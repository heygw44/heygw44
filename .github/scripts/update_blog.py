import feedparser
import re
import html
from datetime import datetime


def clean_html(raw_html: str) -> str:
    """HTML/Markdown íƒœê·¸ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    if not raw_html:
        return ""

    # HTML ì—”í‹°í‹°(&lt;, &gt;, &amp;) í•´ì œ
    text = html.unescape(raw_html)

    # 1) HTML íƒœê·¸ ì œê±°
    text = re.sub(r"<.*?>", "", text)

    # 2) Markdown ë§í¬ [text](url) -> text
    text = re.sub(r"\[([^\]]+)\]\([^)]+\)", r"\1", text)

    # 3) êµµê²Œ/ê¸°ìš¸ì„ í‘œì‹œ ì œê±° **text**, *text*
    text = re.sub(r"\*\*([^*]+)\*\*", r"\1", text)
    text = re.sub(r"\*([^*]+)\*", r"\1", text)

    # 4) ì¸ë¼ì¸ ì½”ë“œ ë°±í‹± ì œê±°
    text = text.replace("`", "")

    # 5) ê³µë°± ì •ë¦¬
    text = re.sub(r"\s+", " ", text)

    return text.strip()


def get_thumbnail(entry) -> str:
    """RSS ì—”íŠ¸ë¦¬ì—ì„œ ì¸ë„¤ì¼ ì´ë¯¸ì§€ URLì„ ì¶”ì¶œ"""
    def _normalize_url(url: str) -> str:
        if not url:
            return url
        url = url.strip()
        if url.startswith("//"):
            url = "https:" + url
        elif url.startswith("http://"):
            url = url.replace("http://", "https://")
        return url

    if hasattr(entry, "media_thumbnail"):
        try:
            url = entry.media_thumbnail[0].get("url")
            url = _normalize_url(url)
            if url:
                return url
        except Exception:
            pass

    if hasattr(entry, "enclosures") and entry.enclosures:
        for enclosure in entry.enclosures:
            if enclosure.get("type", "").startswith("image/"):
                url = _normalize_url(enclosure.get("url"))
                if url:
                    return url

    if hasattr(entry, "description") and entry.description:
        img_match = re.search(r'<img[^>]+src="([^"]+)"', entry.description)
        if img_match:
            url = _normalize_url(img_match.group(1))
            if url:
                return url

    return "https://via.placeholder.com/300x200?text=No+Image"


def format_date(date_str: str) -> str:
    """RSSì˜ ë‚ ì§œë¥¼ YYYY.MM.DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    if not date_str:
        return ""
    try:
        date_obj = datetime.strptime(date_str, "%a, %d %b %Y %H:%M:%S %z")
        return date_obj.strftime("%Y.%m.%d")
    except Exception:
        return date_str


def create_blog_table(feed_url: str, max_posts: int = 6) -> str:
    """RSS í”¼ë“œì—ì„œ ë¸”ë¡œê·¸ ê¸€ì„ ê°€ì ¸ì™€ 3ì—´ HTML í…Œì´ë¸” ìƒì„±"""
    feed = feedparser.parse(feed_url)
    entries = feed.entries[:max_posts]

    if not entries:
        return "<p>ìµœê·¼ ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.</p>"

    table = "<table>\n"

    for i in range(0, len(entries), 3):
        row_entries = entries[i: i + 3]
        while len(row_entries) < 3:
            row_entries.append(None)

        table += "  <tr>\n"

        for entry in row_entries:
            if entry is None:
                table += "    <td></td>\n"
                continue

            thumbnail = get_thumbnail(entry)
            title = clean_html(entry.title)
            link = entry.link
            
            # ë‚ ì§œ í¬ë§·
            pub_date = format_date(entry.get("published", ""))
            
            # description ì²˜ë¦¬
            raw_desc = entry.get("description", "")
            description = clean_html(raw_desc)
            
            # ë¹„êµë¥¼ ìœ„í•´ íŠ¹ìˆ˜ë¬¸ìì™€ ê´„í˜¸ ë“±ì„ ì œê±°í•œ 'ìˆœìˆ˜ í…ìŠ¤íŠ¸' ìƒì„±
            # ì˜ˆ: "[Gradle] ì œëª©" -> "ì œëª©", "Gradle ì œëª©" -> "ì œëª©"
            def normalize_string(s):
                # ëŒ€ê´„í˜¸ ì•ˆì˜ ë‚´ìš©([]) ì œê±°
                s = re.sub(r"\[.*?\]", "", s)
                # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì†Œë¬¸ì ë³€í™˜
                s = re.sub(r"[^\w\s]", "", s).lower()
                return s.replace(" ", "")

            norm_title = normalize_string(title)
            norm_desc = normalize_string(description)

            # 1. ì¼ë°˜ì ì¸ 'ì œëª©ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°' ì œê±°
            if description.lower().startswith(title.lower()):
                description = description[len(title):]
            
            # 2. ì •ê·œí™”ëœ(ê´„í˜¸ ë—€) ì œëª©ì´ ë³¸ë¬¸ ì‹œì‘ê³¼ ì¼ì¹˜í•˜ëŠ” ê²½ìš° ì œê±° (ìŠ¤í¬ë¦°ìƒ· ë¬¸ì œ í•´ê²°)
            elif norm_desc.startswith(norm_title):
                # ë³¸ë¬¸ì—ì„œ ì œëª© ê¸¸ì´ë§Œí¼ ëŒ€ëµì ìœ¼ë¡œ ì˜ë¼ë‚´ê¸° (ì •í™•í•œ ì¸ë±ìŠ¤ ì°¾ê¸° ì–´ë ¤ìš°ë¯€ë¡œ ê¸¸ì´ ì¶”ì •)
                # ì›ë³¸ ì œëª© ê¸¸ì´ë§Œí¼ ìë¥´ë˜, ì•ë¶€ë¶„ì˜ ë‚¨ì€ íŠ¹ìˆ˜ë¬¸ìë“¤ ì œê±°
                if len(description) > len(title):
                     # ì œëª©ê³¼ ìœ ì‚¬í•œ ì•ë¶€ë¶„ì„ ê±´ë„ˆëœ€ (ì¡°ê¸ˆ ë” ë³´ìˆ˜ì ìœ¼ë¡œ 0.8ë°° ê¸¸ì´ë¶€í„° íƒìƒ‰)
                     description = description[len(title):]
            
            # 3. ì•ë¶€ë¶„ì— ë‚¨ì€ íŠ¹ìˆ˜ë¬¸ì(-, :, | ë“±) ë° ê³µë°± ì œê±°
            description = description.lstrip(" -:|[]")
            
            # -----------------------------------------------

            # ê¸¸ì´ ì œí•œ
            max_len = 100
            if len(description) > max_len:
                description = description[:max_len].rstrip() + "..."
            
            # ì œëª©ì—ì„œ ê´„í˜¸ ë“± ì œê±°í•˜ì—¬ alt íƒœê·¸ìš© ì•ˆì „í•œ ì œëª© ìƒì„±
            safe_title = re.sub(r"[\[\]\(\)`]", "", title)

            cell = (
                f'<a href="{link}">'
                f'<img src="{thumbnail}" alt="{safe_title}" width="300" height="200" />'
                f"</a><br/>"
                f'<strong><a href="{link}">{title}</a></strong><br/>'
                f"{description}<br/>"
                f"{pub_date}"
            )

            table += f"    <td>{cell}</td>\n"

        table += "  </tr>\n"

    table += "</table>\n"
    return table


def update_readme(readme_path: str, table_content: str) -> None:
    """README.md íŒŒì¼ì˜ ë§ˆì»¤ ì‚¬ì´ ë‚´ìš©ì„ ìƒˆë¡œìš´ í…Œì´ë¸”ë¡œ ì—…ë°ì´íŠ¸"""
    with open(readme_path, "r", encoding="utf-8") as f:
        content = f.read()

    start_marker = "<!-- BLOG-POST-LIST:START -->"
    end_marker = "<!-- BLOG-POST-LIST:END -->"

    start_idx = content.find(start_marker)
    end_idx = content.find(end_marker)

    if start_idx != -1 and end_idx != -1:
        new_content = (
            content[: start_idx + len(start_marker)]
            + "\n"
            + table_content
            + "\n"
            + content[end_idx:]
        )
        with open(readme_path, "w", encoding="utf-8") as f:
            f.write(new_content)
        print("âœ… README.md updated successfully!")
    else:
        print("âŒ Could not find markers in README.md")


if __name__ == "__main__":
    RSS_FEED_URL = "https://cayman031.tistory.com/rss"
    README_PATH = "README.md"

    print("ğŸ“¡ Fetching blog posts from RSS feed...")
    table = create_blog_table(RSS_FEED_URL, max_posts=6)

    print("ğŸ“ Updating README.md...")
    update_readme(README_PATH, table)