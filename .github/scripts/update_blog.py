import feedparser
import re
from datetime import datetime


def clean_html(raw_html: str) -> str:
    """HTML/Markdown íƒœê·¸ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    if not raw_html:
        return ""

    # 1) HTML íƒœê·¸ ì œê±°
    cleanr = re.compile("<.*?>")
    cleantext = re.sub(cleanr, "", raw_html)

    # 2) Markdown ë§í¬ [text](url) -> text
    cleantext = re.sub(r"\[([^\]]+)\]\([^)]+\)", r"\1", cleantext)

    # 3) êµµê²Œ/ê¸°ìš¸ì„ í‘œì‹œ ì œê±° **text** -> text, *text* -> text
    cleantext = re.sub(r"\*\*([^*]+)\*\*", r"\1", cleantext)
    cleantext = re.sub(r"\*([^*]+)\*", r"\1", cleantext)

    # 4) ì¸ë¼ì¸ ì½”ë“œ ë°±í‹± ì œê±° `code` -> code
    cleantext = cleantext.replace("`", "")

    # 5) ì¤„ë°”ê¿ˆ/ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜
    cleantext = re.sub(r"\s+", " ", cleantext)

    return cleantext.strip()


def get_thumbnail(entry) -> str:
    """RSS ì—”íŠ¸ë¦¬ì—ì„œ ì¸ë„¤ì¼ ì´ë¯¸ì§€ URLì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""

    def _normalize_url(url: str) -> str:
        """í”„ë¡œí† ì½œ/í˜•ì‹ì„ GitHubì—ì„œ ì•ˆì „í•˜ê²Œ ì“¸ ìˆ˜ ìˆë„ë¡ ì •ë¦¬"""
        if not url:
            return url
        url = url.strip()
        if url.startswith("//"):
            url = "https:" + url
        elif url.startswith("http://"):
            url = url.replace("http://", "https://")
        return url

    # ìš°ì„ ìˆœìœ„ 1: media_thumbnail
    if hasattr(entry, "media_thumbnail"):
        try:
            url = entry.media_thumbnail[0].get("url")
            url = _normalize_url(url)
            if url:
                return url
        except Exception:
            pass

    # ìš°ì„ ìˆœìœ„ 2: enclosures
    if hasattr(entry, "enclosures") and entry.enclosures:
        for enclosure in entry.enclosures:
            if enclosure.get("type", "").startswith("image/"):
                url = _normalize_url(enclosure.get("url"))
                if url:
                    return url

    # ìš°ì„ ìˆœìœ„ 3: description ë‚´ ì²« ë²ˆì§¸ <img>
    if hasattr(entry, "description") and entry.description:
        img_match = re.search(r'<img[^>]+src="([^"]+)"', entry.description)
        if img_match:
            url = _normalize_url(img_match.group(1))
            if url:
                return url

    # ëª¨ë‘ ì—†ì„ ê²½ìš° ê¸°ë³¸ ì´ë¯¸ì§€ (ê³µìš© placeholder)
    return "https://via.placeholder.com/300x200?text=No+Image"


def format_date(date_str: str) -> str:
    """RSSì˜ ë‚ ì§œë¥¼ YYYY.MM.DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜"""
    if not date_str:
        return ""

    try:
        # RSS í‘œì¤€ ë‚ ì§œ í˜•ì‹ íŒŒì‹±
        date_obj = datetime.strptime(date_str, "%a, %d %b %Y %H:%M:%S %z")
        return date_obj.strftime("%Y.%m.%d")
    except Exception:
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
        return date_str


def create_blog_table(feed_url: str, max_posts: int = 6) -> str:
    """RSS í”¼ë“œì—ì„œ ë¸”ë¡œê·¸ ê¸€ì„ ê°€ì ¸ì™€ 3ì—´ HTML í…Œì´ë¸” ìƒì„±"""

    # RSS í”¼ë“œ íŒŒì‹±
    feed = feedparser.parse(feed_url)
    entries = feed.entries[:max_posts]  # ìµœì‹  ê¸€ë§Œ ê°€ì ¸ì˜¤ê¸°

    if not entries:
        return "<p>ìµœê·¼ ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.</p>"

    # HTML í…Œì´ë¸” ì‹œì‘
    table = "<table>\n"

    # 3ê°œì”© ë¬¶ì–´ì„œ í–‰ ìƒì„±
    for i in range(0, len(entries), 3):
        row_entries = entries[i: i + 3]

        # 3ê°œ ë¯¸ë§Œì¸ ê²½ìš° Noneìœ¼ë¡œ ì±„ì›Œì„œ 3ì¹¸ ë§ì¶”ê¸°
        while len(row_entries) < 3:
            row_entries.append(None)

        table += "  <tr>\n"

        for entry in row_entries:
            if entry is None:
                table += "    <td></td>\n"
                continue

            # ê° ê¸€ì˜ ì •ë³´ ì¶”ì¶œ
            thumbnail = get_thumbnail(entry)                    # ì¸ë„¤ì¼ ì´ë¯¸ì§€
            title = entry.title                                 # ê¸€ ì œëª©
            link = entry.link                                   # ê¸€ ë§í¬
            description = clean_html(entry.get("description", ""))[:50] + "..."
            pub_date = format_date(entry.get("published", ""))  # ë°œí–‰ì¼

            # alt í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ìˆ˜ë¬¸ì ì œê±°
            safe_title = re.sub(r"[\[\]\(\)`]", "", title)

            # ì…€ ë‚´ìš©: HTMLë§Œ ì‚¬ìš© (ë§ˆí¬ë‹¤ìš´ X)
            cell = (
                f'<a href="{link}">'
                f'<img src="{thumbnail}" alt="{safe_title}" width="300" height="200" />'
                f"</a><br/>"
                f'<strong><a href="{link}">{title}</a></strong><br/>'
                f"{description}<br/>"
                f"{pub_date}"
            )

            table += f"    <td>{cell}</td>\n"

        table += "  </tr>\n"

    table += "</table>\n"
    return table


def update_readme(readme_path: str, table_content: str) -> None:
    """README.md íŒŒì¼ì˜ ë§ˆì»¤ ì‚¬ì´ ë‚´ìš©ì„ ìƒˆë¡œìš´ í…Œì´ë¸”ë¡œ ì—…ë°ì´íŠ¸"""

    # README.md ì½ê¸°
    with open(readme_path, "r", encoding="utf-8") as f:
        content = f.read()

    # ì—…ë°ì´íŠ¸í•  ì˜ì—­ì„ ë‚˜íƒ€ë‚´ëŠ” ë§ˆì»¤
    # README.md ì•ˆì— ë°˜ë“œì‹œ ì•„ë˜ ë‘ ì¤„ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
    # <!-- BLOG-POST-LIST:START -->
    # <!-- BLOG-POST-LIST:END -->
    start_marker = "<!-- BLOG-POST-LIST:START -->"
    end_marker = "<!-- BLOG-POST-LIST:END -->"

    # ë§ˆì»¤ ìœ„ì¹˜ ì°¾ê¸°
    start_idx = content.find(start_marker)
    end_idx = content.find(end_marker)

    # ë§ˆì»¤ê°€ ìˆìœ¼ë©´ ë‚´ìš© êµì²´
    if start_idx != -1 and end_idx != -1:
        new_content = (
            content[: start_idx + len(start_marker)]
            + "\n"
            + table_content
            + "\n"
            + content[end_idx:]
        )

        # README.md íŒŒì¼ì— ì“°ê¸°
        with open(readme_path, "w", encoding="utf-8") as f:
            f.write(new_content)

        print("âœ… README.md updated successfully!")
    else:
        print("âŒ Could not find markers in README.md")
        print("README.md ì— ë‹¤ìŒ ë‘ ë§ˆì»¤ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:")
        print(start_marker)
        print(end_marker)


if __name__ == "__main__":
    # ë³¸ì¸ì˜ Tistory RSS URL
    RSS_FEED_URL = "https://cayman031.tistory.com/rss"
    README_PATH = "README.md"

    print("ğŸ“¡ Fetching blog posts from RSS feed...")
    table = create_blog_table(RSS_FEED_URL, max_posts=6)

    print("ğŸ“ Updating README.md...")
    update_readme(README_PATH, table)
